{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2db464c3",
   "metadata": {},
   "source": [
    "# Weapon Detection using YOLOv8\n",
    "\n",
    "This notebook trains YOLOv8 models for weapon detection using the [Weapon Detection Dataset v2](https://universe.roboflow.com/joao-assalim-xmovq/weapon-2/dataset/2) from Roboflow.\n",
    "\n",
    "## Setup\n",
    "First, we'll install the required libraries and import dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5b82eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import yaml\n",
    "import time\n",
    "import shutil\n",
    "import random\n",
    "from google.colab import files\n",
    "import numpy as np\n",
    "\n",
    "!pip install ultralytics roboflow\n",
    "\n",
    "from ultralytics import YOLO\n",
    "# Set up Roboflow for dataset download\n",
    "from roboflow import Roboflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66dd06a6",
   "metadata": {},
   "source": [
    "## Dataset Functions\n",
    "\n",
    "Functions to download and prepare the weapon detection dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a530f3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_dataset(api_key, workspace=\"joao-assalim-xmovq\", project=\"weapon-2\", version=2):\n",
    "    \"\"\"Download and prepare dataset from Roboflow with proper folder structure.\"\"\"\n",
    "    print(f\"Downloading dataset: {project} (v{version}) from {workspace}...\")\n",
    "    rf = Roboflow(api_key=api_key)\n",
    "    proj = rf.workspace(workspace).project(project)\n",
    "    dataset = proj.version(version).download(\"yolov8\")\n",
    "    dataset_path = dataset.location\n",
    "    \n",
    "    # Fix dataset directory structure if needed\n",
    "    for split in [\"train\", \"test\"]:\n",
    "        split_dir = os.path.join(dataset_path, split)\n",
    "        imgs_dir = os.path.join(split_dir, \"images\")\n",
    "        if os.path.exists(split_dir) and not os.path.exists(imgs_dir):\n",
    "            os.makedirs(imgs_dir, exist_ok=True)\n",
    "            for f in os.listdir(split_dir):\n",
    "                if f.lower().endswith((\".jpg\",\".jpeg\",\".png\",\".bmp\")):\n",
    "                    os.rename(os.path.join(split_dir, f), os.path.join(imgs_dir, f))\n",
    "            print(f\"Fixed {split}/images structure.\")\n",
    "\n",
    "    for split in [\"train\", \"valid\", \"test\"]:\n",
    "        split_dir = os.path.join(dataset_path, split)\n",
    "        if not os.path.isdir(split_dir): continue\n",
    "        lbl_dir = os.path.join(split_dir, \"labels\")\n",
    "        if not os.path.exists(lbl_dir):\n",
    "            has_txt = any(f.endswith(\".txt\") for f in os.listdir(split_dir))\n",
    "            if has_txt:\n",
    "                os.makedirs(lbl_dir, exist_ok=True)\n",
    "                for f in os.listdir(split_dir):\n",
    "                    if f.endswith(\".txt\"):\n",
    "                        os.rename(os.path.join(split_dir, f), os.path.join(lbl_dir, f))\n",
    "                print(f\"Fixed {split}/labels structure.\")\n",
    "\n",
    "    # Create validation split if missing\n",
    "    train_img = os.path.join(dataset_path, \"train\", \"images\")\n",
    "    train_lbl = os.path.join(dataset_path, \"train\", \"labels\")\n",
    "    val_img   = os.path.join(dataset_path, \"valid\", \"images\")\n",
    "    val_lbl   = os.path.join(dataset_path, \"valid\", \"labels\")\n",
    "    \n",
    "    os.makedirs(val_img, exist_ok=True)\n",
    "    os.makedirs(val_lbl, exist_ok=True)\n",
    "    \n",
    "    if len(os.listdir(val_img)) == 0 and os.path.exists(train_img):\n",
    "        imgs = [f for f in os.listdir(train_img) if f.lower().endswith((\".jpg\",\".png\",\".bmp\"))]\n",
    "        if not imgs:\n",
    "            print(\"No train images to split for validation.\")\n",
    "        else:\n",
    "            cnt = max(1, int(len(imgs)*0.1))\n",
    "            random.seed(42)\n",
    "            chosen = random.sample(imgs, cnt)\n",
    "            for im in chosen:\n",
    "                shutil.copy2(os.path.join(train_img, im), os.path.join(val_img, im))\n",
    "                lbl = os.path.splitext(im)[0] + \".txt\"\n",
    "                if os.path.exists(os.path.join(train_lbl, lbl)):\n",
    "                    shutil.copy2(os.path.join(train_lbl, lbl), os.path.join(val_lbl, lbl))\n",
    "            print(f\"Created valid set with {cnt} images.\")\n",
    "\n",
    "    # Fix YAML paths\n",
    "    yaml_path = os.path.join(dataset_path, \"data.yaml\")\n",
    "    if os.path.exists(yaml_path):\n",
    "        with open(yaml_path) as rf:\n",
    "            data = yaml.safe_load(rf)\n",
    "        modified = False\n",
    "        for k in [\"train\",\"val\",\"test\"]:\n",
    "            if k in data and data[k] and \"/images\" not in data[k]:\n",
    "                data[k] = os.path.join(data[k], \"images\")\n",
    "                modified = True\n",
    "        if modified:\n",
    "            with open(yaml_path, \"w\") as wf:\n",
    "                yaml.dump(data, wf)\n",
    "            print(\"Updated data.yaml image paths.\")\n",
    "\n",
    "    print(f\"Dataset ready at: {dataset_path}\")\n",
    "    return dataset_path\n",
    "\n",
    "def display_dataset_info(dataset_path):\n",
    "    \"\"\"Print class names and image counts per split.\"\"\"\n",
    "    yaml_path = os.path.join(dataset_path, \"data.yaml\")\n",
    "    with open(yaml_path) as f:\n",
    "        d = yaml.safe_load(f)\n",
    "    print(f\"- Classes ({len(d['names'])}): {d['names']}\")\n",
    "    for split in [\"train\",\"valid\",\"test\"]:\n",
    "        p = os.path.join(dataset_path, split, \"images\")\n",
    "        print(f\"- {split} images: {len(os.listdir(p)) if os.path.isdir(p) else 0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5d6a0d",
   "metadata": {},
   "source": [
    "## Training and Evaluation Functions\n",
    "\n",
    "Functions to train the YOLOv8 model and evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89c05b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_yolo(dataset_path, model_size='s', epochs=50, batch_size=16, img_size=640):\n",
    "    \"\"\"Train YOLOv8 model with T4-friendly defaults.\"\"\"\n",
    "    print(\"Checking GPU…\")\n",
    "    !nvidia-smi\n",
    "    if model_size=='x': batch_size=min(batch_size,8)\n",
    "    elif model_size=='l': batch_size=min(batch_size,12)\n",
    "    elif model_size=='m': batch_size=min(batch_size,16)\n",
    "    print(f\"Batch size set to {batch_size} for yolov8{model_size}\")\n",
    "    model = YOLO(f\"yolov8{model_size}.pt\")\n",
    "    data_yaml = os.path.join(dataset_path, \"data.yaml\")\n",
    "    output_dir = \"/content/yolov8_weapon_detection\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(f\"Training yolov8{model_size} for {epochs} epochs…\")\n",
    "    start = time.time()\n",
    "    res = model.train(\n",
    "        data=data_yaml,\n",
    "        epochs=epochs,\n",
    "        batch=batch_size,\n",
    "        imgsz=img_size,\n",
    "        project=output_dir,\n",
    "        name=f\"yolov8{model_size}_weapon_detection2\",\n",
    "        device=0 if torch.cuda.is_available() else \"cpu\",\n",
    "        cache=True,\n",
    "        amp=True,\n",
    "        plots=True,\n",
    "        verbose=True\n",
    "    )\n",
    "    print(f\"Done in {(time.time()-start)/60:.2f} min\")\n",
    "    return model, output_dir\n",
    "\n",
    "def evaluate_model(model, dataset_path):\n",
    "    \"\"\"Evaluate on valid set and print metrics.\"\"\"\n",
    "    data_yaml = os.path.join(dataset_path, \"data.yaml\")\n",
    "    print(\"Running evaluation…\")\n",
    "    try:\n",
    "        # Updated evaluation to handle API changes\n",
    "        results = model.val(data=data_yaml, verbose=True)\n",
    "        # Modern Ultralytics stores metrics differently\n",
    "        if hasattr(results, 'box'):\n",
    "            m = results.box\n",
    "            print(f\"P: {m.p:.4f}, R: {m.r:.4f}, mAP50: {m.map50:.4f}, mAP50-95: {m.map:.4f}\")\n",
    "        else:\n",
    "            # Fallback for newer versions\n",
    "            metrics = results.results_dict\n",
    "            print(f\"P: {metrics.get('precision', 0):.4f}, R: {metrics.get('recall', 0):.4f}\")\n",
    "            print(f\"mAP50: {metrics.get('metrics/mAP50(B)', 0):.4f}, mAP50-95: {metrics.get('metrics/mAP50-95(B)', 0):.4f}\")\n",
    "        return results\n",
    "    except AttributeError as e:\n",
    "        print(f\"Evaluation error with current metrics API: {e}\")\n",
    "        print(\"Using alternative metrics access...\")\n",
    "        # Try alternative approach for latest YOLO versions\n",
    "        results = model.val(data=data_yaml)\n",
    "        for metric_name, value in results.results_dict.items():\n",
    "            if isinstance(value, (int, float)):\n",
    "                print(f\"{metric_name}: {value:.4f}\")\n",
    "        return results\n",
    "\n",
    "def download_model(model_dir):\n",
    "    \"\"\"Download best.pt and last.pt via Colab.\"\"\"\n",
    "    for name in (\"best.pt\",\"last.pt\"):\n",
    "        p = os.path.join(model_dir, \"weights\", name)\n",
    "        if os.path.exists(p):\n",
    "            print(f\"Downloading {name}…\")\n",
    "            files.download(p)\n",
    "        else:\n",
    "            print(f\"{name} not found at {p}\")\n",
    "\n",
    "def export_model(model, fmt='pt'):\n",
    "    \"\"\"Export and download the model in the given format.\"\"\"\n",
    "    print(f\"Exporting to {fmt}…\")\n",
    "    path = model.export(format=fmt, imgsz=640)\n",
    "    print(\"Exported to\", path)\n",
    "    files.download(str(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdd3f35",
   "metadata": {},
   "source": [
    "## Main Function\n",
    "\n",
    "The main function to run the entire workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874138b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    print(\"=== YOLO Weapon Detection Workflow ===\")\n",
    "    key = input(\"Roboflow API key: \")\n",
    "    try:\n",
    "        ds = download_dataset(key)\n",
    "        display_dataset_info(ds)\n",
    "        sz = input(\"Model size (n/s/m/l/x) [s]: \") or \"s\"\n",
    "        ep = int(input(\"Epochs [50]: \") or 50)\n",
    "        bs = int(input(\"Batch size [16]: \") or 16)\n",
    "        img = int(input(\"Image size [640]: \") or 640)\n",
    "\n",
    "        model, out = train_yolo(ds, sz, ep, bs, img)\n",
    "\n",
    "        model_dir = os.path.join(out, f\"yolov8{sz}_weapon_detection2\")\n",
    "        try:\n",
    "            evaluate_model(model, ds)\n",
    "        except Exception as e:\n",
    "            print(\"⚠️  Eval error:\", e)\n",
    "            print(\"→ Skipping evaluation.\")\n",
    "\n",
    "        download_model(model_dir)\n",
    "\n",
    "        if input(\"Export to pt? (y/n) [y]: \").lower() in (\"\",\"y\"):\n",
    "            export_model(model, \"pt\")\n",
    "\n",
    "        print(\"✅ Workflow complete.\")\n",
    "    except Exception as e:\n",
    "        print(\"‼️  ERROR:\", e)\n",
    "        import traceback; traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
